\documentclass[addpoints]{exam}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{venndiagram}
\usepackage{graphicx}

% Header and footer.
\pagestyle{headandfoot}
\runningheadrule
\runningfootrule
\runningheader{HW1}{Linear Algebra}{}
\runningfooter{}{Page \thepage\ of \numpages}{}
\firstpageheader{}{}{}

\boxedpoints
\printanswers
\qformat{} %Comment this to number questions, uncomment this to not number questions

\newcommand\union\cup
\newcommand\inter\cap

\title{Linear Algebra\\ Homework 2 part i}
\author{Ali Muhammad Asad}

\begin{document}
\maketitle
\begin{sloppypar}
\section*{\textbf{Chapter 1 : Linear Equations and Matrices}}

\subsection*{\textbf{Ex Set 1.3 : Matrices and Matrix Operations}}

\begin{questions}
    \question
    \textbf{4. } Consider the matrices
    $$ A = \begin{bmatrix}
        3 & 0 \\ -1 & 2 \\ 1 & 1
    \end{bmatrix},\; B = \begin{bmatrix}
        4 & -1 \\ 0 & 2
    \end{bmatrix},\; C = \begin{bmatrix}
        1 & 4 & 2 \\ 3 & 1 & 5
    \end{bmatrix},\; D = \begin{bmatrix}
        1 & 5 & 2 \\ -1 &  0 & 1 \\ 3 & 2 & 4
    \end{bmatrix},\; E = \begin{bmatrix}
        6 & 1 & 3 \\ -1 & 1 & 2 \\ 4 & 1 & 3
    \end{bmatrix} $$
    Using these matrices, compute the following: \\ 
    (a) $ 2A^T + C $ \hspace{10mm} (b) $ D^T - E^T $ \hspace{10mm} (c) $ (D - E)^T $ \hspace{10mm} (d) $ B^T + 5C^T $
    
    \vspace{2mm} (e) $ \frac{1}{2}C^T - \frac{1}{4}A $ \hspace{7.5mm} (f) $ B - B^T $ \hspace{13mm} (g) $ 2E^T - 3D^T $  \hspace{7mm} (h) $ (2E^T - 3D^T)^T $
    \begin{solution}

        (a) $ = 2\begin{bmatrix}
            3 & -1 & 1 \\ 0 & 2 & 1
        \end{bmatrix} + \begin{bmatrix}
            1 & 4 & 2 \\ 3 & 1 & 5
        \end{bmatrix} = \begin{bmatrix}
            6 & -2 & 2 \\ 0 & 4 & 2
        \end{bmatrix} + \begin{bmatrix}
            1 & 4 & 2 \\ 3 & 1 & 5
        \end{bmatrix} = \begin{bmatrix}
            7 & 2 & 4 \\ 3 & 5 & 7
        \end{bmatrix} $

        (b) $ = \begin{bmatrix}
            1 & -1 & 3 \\ 5 & 0 & 2 \\ 2 & 1 & 4
        \end{bmatrix} - \begin{bmatrix}
            6 & -1 & 4 \\ 1 & 1 & 1 \\ 3 & 2 & 3
        \end{bmatrix} = \begin{bmatrix}
            -5 & 0 & -1 \\ 4 & -1 & 1 \\ -1 & -1 & 1
        \end{bmatrix}$

        (c) $ = \Biggl(\begin{bmatrix}
            1 & 5 & 2 \\ -1 &  0 & 1 \\ 3 & 2 & 4
        \end{bmatrix} - \begin{bmatrix}
            6 & 1 & 3 \\ -1 & 1 & 2 \\ 4 & 1 & 3
        \end{bmatrix}\Biggr)^T = \Biggl( \begin{bmatrix}
            -5 & 4 & -1 \\ 0 & -1 & -1 \\ -1 & 1 & 1
        \end{bmatrix} \Biggr)^T = \begin{bmatrix}
            -5 & 0 & -1 \\ 4 & -1 & 1 \\ -1 & -1 & 1
        \end{bmatrix} $

        (d) Cannot be computed since the matrix $ B^T $ and $ C^T $ have different orders, so addition is not defined.

        (e) $ = \frac{1}{2}\begin{bmatrix}
            1 & 3 \\ 4 & 1 \\ 2 & 5
        \end{bmatrix} -\frac{1}{4}\begin{bmatrix}
            3 & 0 \\ -1 & 2 \\ 1 & 1
        \end{bmatrix} = \begin{bmatrix}
            -\frac{1}{4} & \frac{3}{2} \\ 
            \frac{9}{4} & 0 \\ \frac{3}{4} & \frac{9}{4}
        \end{bmatrix}$

        (f) $ = \begin{bmatrix}
            4 & -1 \\ 0 & 2
        \end{bmatrix} - \begin{bmatrix}
            4 & 0 \\ -1 & 2
        \end{bmatrix} = \begin{bmatrix}
            0 & -1 \\ 1 & 0
        \end{bmatrix}$

        (g) $ = 2\begin{bmatrix}
            6 & -1 & 4 \\ 1 & 1 & 1 \\ 3 & 2 & 3
        \end{bmatrix} - 3\begin{bmatrix}
            1 & -1 & 3 \\ 5 & 0 & 2 \\ 2 & 1 & 4
        \end{bmatrix} = \begin{bmatrix}
            9 & 1 & -1 \\ 
            -13 & 2 & -4 \\ 
            0 & 1 & -6
        \end{bmatrix}$

        (h) $ = \Biggl( 2\begin{bmatrix}
            6 & -1 & 4 \\ 1 & 1 & 1 \\ 3 & 2 & 3
        \end{bmatrix} - 3\begin{bmatrix}
            1 & -1 & 3 \\ 5 & 0 & 2 \\ 2 & 1 & 4
        \end{bmatrix} \Biggr)^T = \Biggl( \begin{bmatrix}
            9 & 1 & -1 \\ 
            -13 & 2 & -4 \\ 
            0 & 1 & -6
        \end{bmatrix} \Biggr)^T = \begin{bmatrix}
            9 & -13 & 0 \\ 1 & 2 & 1 \\ -1 & -4 & -6
        \end{bmatrix} $

    \end{solution}
\end{questions}


\subsection*{\textbf{Ex Set 1.4 : Inverses; Rules of Matrix Arithmetic}}
% \vspace{1mm}
\begin{questions}
    \question
    \textbf{11. } Find the inverse of $ \begin{bmatrix}
        \cos\theta & \sin\theta \\ 
        -\sin\theta & \cos\theta
    \end{bmatrix} $
    \begin{solution}
        Determinant of the matrix $ = (\cos\theta \times \cos\theta) - (-\sin\theta \times \sin\theta)$ \\ 
        Det $ = \cos^2\theta + \sin^2\theta \implies $ Det $ = 1 $ \\ 
        Adjoint $ = \begin{bmatrix}
            \cos\theta & -\sin\theta \\ 
            \sin\theta & \cos\theta
        \end{bmatrix} $ \\  
        Inverse = $ \frac{\text{Adjoint}}{\text{Determinant}} \implies $ Inverse $ = \begin{bmatrix}
            \cos\theta & -\sin\theta \\ 
            \sin\theta & \cos\theta
        \end{bmatrix} $
    \end{solution}

    \question
    \textbf{13. } Consider the matrix $$ A = \begin{bmatrix}
        a_{11} & 0 & \cdots & 0 \\ 
        0 & a_{22} & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\ 
        0 & 0 & \cdots & a_{nn}
    \end{bmatrix} $$
    where $ a_{11}a_{22}\cdots a_{nn} \neq 0 $. Show that $A$ is invertible, and find its inverse.
    \begin{solution}
        $A = $ diag$ (a_{11}, a_{22}, ..., a_{nn}) $. \\ 
        For $A$ to be invertible, det($A$) $ \neq 0 $. \\ 
        The deterinant of any diagonal matrix is the product of its daigonals. Then det($A$) $ = a_{11}a_{22}...a_{nn} \neq 0 $ since $ a_{11}a_{22}...a_{nn} \neq 0 $. Therefore, the inverse of $A$ exists. 
        
        For $A^{-1}$ to exist, $ AA^{-1} = I $ and $ A^{-1}A = I $ must be true by the definition. \\ 
        Let $ A^{-1} = \begin{bmatrix}
            \alpha_{11} & \alpha_{12} & ... & \alpha_{1n} \\ 
            \alpha_{21} & \alpha_{22} & ... & \alpha_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            \alpha_{n1} & \alpha_{n2} & ... & \alpha_{nn}
        \end{bmatrix} $ 
        
        \vspace{5mm}
        Then $ AA^{-1} = I \implies \begin{bmatrix}
            a_{11} & 0 & \cdots & 0 \\ 
            0 & a_{22} & \cdots & 0 \\
            \vdots & \vdots & & \vdots \\ 
            0 & 0 & \cdots & a_{nn}
        \end{bmatrix}\begin{bmatrix}
            \alpha_{11} & \alpha_{12} & ... & \alpha_{1n} \\ 
            \alpha_{21} & \alpha_{22} & ... & \alpha_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            \alpha_{n1} & \alpha_{n2} & ... & \alpha_{nn}
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 & ... & 0 \\ 0 & 1 & ... & 0 \\ \vdots & \vdots & & \vdots \\ 0 & 0 & ... & 1
        \end{bmatrix} $ \\ 
        $ \implies \begin{bmatrix}
            a_{11}\alpha_{11} & a_{11}\alpha_{12} & ... & a_{11}\alpha_{1n} \\ 
            a_{22}\alpha_{21} & a_{22}\alpha_{22} & ... & a_{22}\alpha_{2n} \\ \vdots & \vdots & & \vdots \\ 
            a_{nn}\alpha_{n1} & a_{nn}\alpha_{n2} & ... & a_{nn}\alpha_{nn}
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 & ... & 0 \\ 0 & 1 & ... & 0 \\ \vdots & \vdots & & \vdots \\ 0 & 0 & ... & 1
        \end{bmatrix} $ \\ 
        By comparison, $ a_{11}\alpha_{11} = 1 \implies \alpha_{11} = \frac{1}{a_{11}}, \\ a_{11}\alpha_{12} = 0 \implies \alpha_{12} = 0 $ as $ a_{11} \neq 0 $. The same argument holds for $ \alpha_{12} ... \alpha_{1n}$. Similarly, $ a_{22} \neq 0 \therefore \alpha_{22} = \frac{1}{a_{22}} $ however $ \alpha_{21} = 0 $ and $ \alpha_{23} ... \alpha_{2n} = 0 $. 
        
        Then we can conclude that $ \alpha_{ij} = \frac{1}{a_{ij}} \; \forall i = j \; | \; i, j \in \mathbb{N} $ where $ i \leq n $ and $ j \leq n $ \\ and $ \alpha_{ij} = 0 \; \forall i \neq j \; | \; i, j \in \mathbb{N} $ where $ i \leq n $ and $ j \neq n $.

        Then $ A^{-1} = \begin{bmatrix}
            \frac{1}{a_{11}} & 0 & ... & 0 \\ 
            0 & \frac{1}{a_{22}} & ... & 0 \\ 
            0 & 0 & ... &  \frac{1}{a_{nn}}
        \end{bmatrix} $
    \end{solution}
    
    \question
    \textbf{15. } (a) Show that a matrix with a row of zeroes cannot have an inverse. 


    \hspace{7.5mm} (b) Show that a matrix with a column of zeroes cannot have an inverse.
    \begin{solution}
        
        (a) Let $ A = \begin{bmatrix}
            0 & 0 & ... & 0 \\ a_{21} & a_{22} & ... & a_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            a_{m1} & a_{m2} & ... & a_{mn}
        \end{bmatrix} $. Let $ B $ be the inverse of $A$. Then $ AB = I $ should hold true. However, we previously proved that if a matrix $ A $ has a row of zeroes, then $ AB $ also has a row of zeroes. Hence, we know that $ AB \neq I $ since $ AB $ has a row of zeroes. So we can conclude that $A$ does not have an inverse if $A$ has a row of zeroes.

        (b) Let $ A = \begin{bmatrix}
            0 & a_{12} & ... & a_{1n} \\ 
            0 & a_{22} & ... & a_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            0 & a_{m2} & ... & a_{nn}
        \end{bmatrix} $ where $A$ has a column of zeroes. \\ 
        Let $B = \begin{bmatrix}
            b_{11} & b_{12} & ... & b_{1n} \\ 
            b_{21} & b_{22} & ... & b_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            b_{n1} & b_{n2} & ... & b_{nn}
        \end{bmatrix}$ be the inverse of $A$, then $ AB = I $ and $ BA = I $ should hold true. \\ 
        Then $ BA = \begin{bmatrix}
            b_{11} & b_{12} & ... & b_{1n} \\ 
            b_{21} & b_{22} & ... & b_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            b_{n1} & b_{n2} & ... & b_{nn}
        \end{bmatrix} \begin{bmatrix}
            0 & a_{12} & ... & a_{1n} \\ 
            0 & a_{22} & ... & a_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            0 & a_{n2} & ... & a_{nn}
        \end{bmatrix} \\ 
        = \begin{bmatrix}
            b_{11}0 + b_{12}0 + ... + b_{1n}0 & b_{11}a_{12} + b_{12}a_{22} + ... + b_{1n}a_{n2} & ... & b_{11}a_{1n} + b_{12}a_{2n} + ... + b_{1n}a_{nn} \\ 
            b_{21}0 + b_{22}0 + ... + b_{2n}0 & b_{21}a_{12} + b_{22}a_{22} + ... + b_{2n}a_{n2} & ... & b_{21}a_{1n} + b_{22}a_{2n} + ... + b_{2n}a_{nn} \\ 
            \vdots & \vdots & & \vdots \\ 
            b_{n1}0 + b_{n2}0 + ... + b_{nn}0 & b_{n1}a_{12} + b_{n2}a_{22} + ... + b_{nn}a_{n2} & ... & b_{n1}a_{1n} + b_{n2}a_{2n} + ... + b_{nn}a_{nn}
        \end{bmatrix} \\ 
        = \begin{bmatrix}
            0 & b_{11}a_{12} + b_{12}a_{22} + ... + b_{1n}a_{n2} & ... & b_{11}a_{1n} + b_{12}a_{2n} + ... + b_{1n}a_{nn} \\ 
            0 & b_{21}a_{12} + b_{22}a_{22} + ... + b_{2n}a_{n2} & ... & b_{21}a_{1n} + b_{22}a_{2n} + ... + b_{2n}a_{nn} \\ 
            \vdots & \vdots & & \vdots \\ 
            0 & b_{n1}a_{12} + b_{n2}a_{22} + ... + b_{nn}a_{n2} & ... & b_{n1}a_{1n} + b_{n2}a_{2n} + ... + b_{nn}a_{nn}
        \end{bmatrix} $.

        Then $BA$ also has a column of zeroes which implies $ BA \neq I $. Hence if $A$ has a column of zeroes, then $BA$ also has a column of zeroes, therefore, $A$ does not have an inverse.
    \end{solution}

    \question
    \textbf{16. } Is the sum of two invertible matrices necessarily invertible? [is not, why?]
    \begin{solution}
        No, the sum of two invertible matrices may not necessarily be invertible. Consider two invertible matrices $A$ and $B$. It is quite possible that the $A + B$ may produce a matrix with a row of zeroes, or a colummn of zeroes, in which case the resultant matrix is no longer invertible. \\ 
        Consider, $A = I$ which is invertible [the identity matrix is invertible], and $B = -I$, which is also invertible [multiplying an invertible matrix with a non-zero constant, the resultant matrix is also invertible]. Then $ A + B = 0$ that is, the sum of $A$ and $B$ produces a 0 matrix which is not invertible. Hence proved. 
    \end{solution}

    \question
    \textbf{17. } Let $A$ and $B$ be square matrices such that $ AB = 0 $. Show that if $A$ is invertible, then $B$ = 0. 
    \begin{solution}
        If $A$ is invertible, then $ A^{-1}A = I $ is true. Then premultiplying $ A^{-1} $ on both sides, we get $ A^{-1}AB = A^{-1}0 \implies IB = 0 $ as a matrix times the 0 matrix is equal to the zero matrix. Then $ IB = 0 \implies B = 0 $ as $ IB = B $. Hence shown that $B = 0$ if $A$ is invertible and $ AB = 0 $. [If $A$ is invertible, then $A \neq 0$ as 0 is not invertible] 
    \end{solution}
    \pagebreak
    \question
    \textbf{29. } (a) Show that if $A$ is invertible and $ AB = AC $, then $ B = C $.


    \hspace{7.5mm} (b) Explain why part (a) and Example 3 (from the book) do not contradict one another.
    \begin{solution}
        
        (a) If $A$ is invertible, and $ AB = AC $ then $ B = C $. \\ 
        If $A$ is invertible, then $ A^{-1}A = I $ holds true. Then premultiplying by $ A^{-1} $ we get $ A^{-1}AB = A^{-1}AC \implies IB = IC \implies B = C$ as $ IA = A $.

        (b) They donot contradict one another as $A$ was not invertible in Example 3. So when $A$ is not invertible, then the operation $ A^{-1}A $ does not hold as it does not exist. Therefore, $A$ cannot be cancelled from both sides. \\ 
        Since $A$ is invertible over here, it can be cancelled, therefore, it does not contradict the example.
    \end{solution}

\end{questions}

\end{sloppypar}

\end{document}