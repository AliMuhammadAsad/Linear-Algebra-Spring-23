\documentclass[addpoints]{exam}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{venndiagram}
\usepackage{graphicx}

% Header and footer.
\pagestyle{headandfoot}
\runningheadrule
\runningfootrule
\runningheader{HW1}{Linear Algebra}{}
\runningfooter{}{Page \thepage\ of \numpages}{}
\firstpageheader{}{}{}

\boxedpoints
\printanswers
\qformat{} %Comment this to number questions, uncomment this to not number questions

\newcommand\union\cup
\newcommand\inter\cap

\title{Linear Algebra\\ Homework 1}
\author{Ali Muhammad Asad}

\begin{document}
\maketitle
\section*{\textbf{Chapter 1 : Linear Equations and Matrices}}

\subsection*{\textbf{Ex Set 1.3 : Matrices and Matrix Operations}}

\begin{questions}
    \question
    \textbf{4. } Consider the matrices
    $$ A = \begin{bmatrix}
        3 & 0 \\ -1 & 2 \\ 1 & 1
    \end{bmatrix},\; B = \begin{bmatrix}
        4 & -1 \\ 0 & 2
    \end{bmatrix},\; C = \begin{bmatrix}
        1 & 4 & 2 \\ 3 & 1 & 5
    \end{bmatrix},\; D = \begin{bmatrix}
        1 & 5 & 2 \\ -1 &  0 & 1 \\ 3 & 2 & 4
    \end{bmatrix},\; E = \begin{bmatrix}
        6 & 1 & 3 \\ -1 & 1 & 2 \\ 4 & 1 & 3
    \end{bmatrix} $$
    Using these matrices, compute the following: \\ 
    (a) $ 2A^T + C $ \hspace{10mm} (b) $ D^T - E^T $ \hspace{10mm} (c) $ (D - E)^T $ \hspace{10mm} (d) $ B^T + 5C^T $
    
    \vspace{2mm} (e) $ \frac{1}{2}C^T - \frac{1}{4}A $ \hspace{7.5mm} (f) $ B - B^T $ \hspace{13mm} (g) $ 2E^T - 3D^T $  \hspace{7mm} (h) $ (2E^T - 3D^T)^T $
    \begin{solution}

        (a) $ = 2\begin{bmatrix}
            3 & -1 & 1 \\ 0 & 2 & 1
        \end{bmatrix} + \begin{bmatrix}
            1 & 4 & 2 \\ 3 & 1 & 5
        \end{bmatrix} = \begin{bmatrix}
            6 & -2 & 2 \\ 0 & 4 & 2
        \end{bmatrix} + \begin{bmatrix}
            1 & 4 & 2 \\ 3 & 1 & 5
        \end{bmatrix} = \begin{bmatrix}
            7 & 2 & 4 \\ 3 & 5 & 7
        \end{bmatrix} $

        (b) $ = \begin{bmatrix}
            1 & -1 & 3 \\ 5 & 0 & 2 \\ 2 & 1 & 4
        \end{bmatrix} - \begin{bmatrix}
            6 & -1 & 4 \\ 1 & 1 & 1 \\ 3 & 2 & 3
        \end{bmatrix} = \begin{bmatrix}
            -5 & 0 & -1 \\ 4 & -1 & 1 \\ -1 & -1 & 1
        \end{bmatrix}$

        (c) $ = \Biggl(\begin{bmatrix}
            1 & 5 & 2 \\ -1 &  0 & 1 \\ 3 & 2 & 4
        \end{bmatrix} - \begin{bmatrix}
            6 & 1 & 3 \\ -1 & 1 & 2 \\ 4 & 1 & 3
        \end{bmatrix}\Biggr)^T = \Biggl( \begin{bmatrix}
            -5 & 4 & -1 \\ 0 & -1 & -1 \\ -1 & 1 & 1
        \end{bmatrix} \Biggr)^T = \begin{bmatrix}
            -5 & 0 & -1 \\ 4 & -1 & 1 \\ -1 & -1 & 1
        \end{bmatrix} $

        (d) Cannot be computed since the matrix $ B^T $ and $ C^T $ have different orders, so addition is not defined.

        (e) $ = \frac{1}{2}\begin{bmatrix}
            1 & 3 \\ 4 & 1 \\ 2 & 5
        \end{bmatrix} -\frac{1}{4}\begin{bmatrix}
            3 & 0 \\ -1 & 2 \\ 1 & 1
        \end{bmatrix} = \begin{bmatrix}
            -\frac{1}{4} & \frac{3}{2} \\ 
            \frac{9}{4} & 0 \\ \frac{3}{4} & \frac{9}{4}
        \end{bmatrix}$

        (f) $ = \begin{bmatrix}
            4 & -1 \\ 0 & 2
        \end{bmatrix} - \begin{bmatrix}
            4 & 0 \\ -1 & 2
        \end{bmatrix} = \begin{bmatrix}
            0 & -1 \\ 1 & 0
        \end{bmatrix}$

        (g) $ = 2\begin{bmatrix}
            6 & -1 & 4 \\ 1 & 1 & 1 \\ 3 & 2 & 3
        \end{bmatrix} - 3\begin{bmatrix}
            1 & -1 & 3 \\ 5 & 0 & 2 \\ 2 & 1 & 4
        \end{bmatrix} = \begin{bmatrix}
            9 & 1 & -1 \\ 
            -13 & 2 & -4 \\ 
            0 & 1 & -6
        \end{bmatrix}$

        (h) $ = \Biggl( 2\begin{bmatrix}
            6 & -1 & 4 \\ 1 & 1 & 1 \\ 3 & 2 & 3
        \end{bmatrix} - 3\begin{bmatrix}
            1 & -1 & 3 \\ 5 & 0 & 2 \\ 2 & 1 & 4
        \end{bmatrix} \Biggr)^T = \Biggl( \begin{bmatrix}
            9 & 1 & -1 \\ 
            -13 & 2 & -4 \\ 
            0 & 1 & -6
        \end{bmatrix} \Biggr)^T = \begin{bmatrix}
            9 & -13 & 0 \\ 1 & 2 & 1 \\ -1 & -4 & -6
        \end{bmatrix} $

    \end{solution}
\end{questions}


\subsection*{\textbf{Ex Set 1.4 : Inverses; Rules of Matrix Arithmetic}}
% \vspace{1mm}
\begin{questions}
    \question
    \textbf{11. } Find the inverse of $ \begin{bmatrix}
        \cos\theta & \sin\theta \\ 
        -\sin\theta & \cos\theta
    \end{bmatrix} $
    \begin{solution}
        Determinant of the matrix $ = (\cos\theta \times \cos\theta) - (-\sin\theta \times \sin\theta)$ \\ 
        Det $ = \cos^2\theta + \sin^2\theta \implies $ Det $ = 1 $ \\ 
        Adjoint $ = \begin{bmatrix}
            \cos\theta & -\sin\theta \\ 
            \sin\theta & \cos\theta
        \end{bmatrix} $ \\  
        Inverse = $ \frac{\text{Adjoint}}{\text{Determinant}} \implies $ Inverse $ = \begin{bmatrix}
            \cos\theta & -\sin\theta \\ 
            \sin\theta & \cos\theta
        \end{bmatrix} $
    \end{solution}

    \question
    \textbf{13. } Consider the matrix $$ A = \begin{bmatrix}
        a_{11} & 0 & \cdots & 0 \\ 
        0 & a_{22} & \cdots & 0 \\
        \vdots & \vdots & & \vdots \\ 
        0 & 0 & \cdots & a_{nn}
    \end{bmatrix} $$
    where $ a_{11}a_{22}\cdots a_{nn} \neq 0 $. Show that $A$ is invertible, and find its inverse.
    \begin{solution}
        $A = $ diag$ (a_{11}, a_{22}, ..., a_{nn}) $. \\ 
        For $A$ to be invertible, det($A$) $ \neq 0 $. \\ 
        The deterinant of any diagonal matrix is the product of its daigonals. Then det($A$) $ = a_{11}a_{22}...a_{nn} \neq 0 $ since $ a_{11}a_{22}...a_{nn} \neq 0 $. Therefore, the inverse of $A$ exists. 
        
        For $A^{-1}$ to exist, $ AA^{-1} = I $ and $ A^{-1}A = I $ must be true by the definition. \\ 
        Let $ A^{-1} = \begin{bmatrix}
            \alpha_{11} & \alpha_{12} & ... & \alpha_{1n} \\ 
            \alpha_{21} & \alpha_{22} & ... & \alpha_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            \alpha_{n1} & \alpha_{n2} & ... & \alpha_{nn}
        \end{bmatrix} $ 
        
        \vspace{5mm}
        Then $ AA^{-1} = I \implies \begin{bmatrix}
            a_{11} & 0 & \cdots & 0 \\ 
            0 & a_{22} & \cdots & 0 \\
            \vdots & \vdots & & \vdots \\ 
            0 & 0 & \cdots & a_{nn}
        \end{bmatrix}\begin{bmatrix}
            \alpha_{11} & \alpha_{12} & ... & \alpha_{1n} \\ 
            \alpha_{21} & \alpha_{22} & ... & \alpha_{2n} \\ 
            \vdots & \vdots & & \vdots \\ 
            \alpha_{n1} & \alpha_{n2} & ... & \alpha_{nn}
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 & ... & 0 \\ 0 & 1 & ... & 0 \\ \vdots & \vdots & & \vdots \\ 0 & 0 & ... & 1
        \end{bmatrix} $ \\ 
        $ \implies \begin{bmatrix}
            a_{11}\alpha_{11} & a_{11}\alpha_{12} & ... & a_{11}\alpha_{1n} \\ 
            a_{22}\alpha_{21} & a_{22}\alpha_{22} & ... & a_{22}\alpha_{2n} \\ \vdots & \vdots & & \vdots \\ 
            a_{nn}\alpha_{n1} & a_{nn}\alpha_{n2} & ... & a_{nn}\alpha_{nn}
        \end{bmatrix} = \begin{bmatrix}
            1 & 0 & ... & 0 \\ 0 & 1 & ... & 0 \\ \vdots & \vdots & & \vdots \\ 0 & 0 & ... & 1
        \end{bmatrix} $ \\ 
        By comparison, $ a_{11}\alpha_{11} = 1 \implies \alpha_{11} = \frac{1}{a_{11}}, \\ a_{11}\alpha_{12} = 0 \implies \alpha_{12} = 0 $ as $ a_{11} \neq 0 $. The same argument holds for $ \alpha_{12} ... \alpha_{1n}$. Similarly, $ a_{22} \neq 0 \therefore \alpha_{22} = \frac{1}{a_{22}} $ however $ \alpha_{21} = 0 $ and $ \alpha_{23} ... \alpha_{2n} = 0 $. 
        
        Then we can conclude that $ \alpha_{ij} = \frac{1}{a_{ij}} \; \forall i = j \; | \; i, j \in \mathbb{N} $ where $ i \leq n $ and $ j \leq n $ \\ and $ \alpha_{ij} = 0 \; \forall i \neq j \; | \; i, j \in \mathbb{N} $ where $ i \leq n $ and $ j \neq n $.

        Then $ A^{-1} = \begin{bmatrix}
            \frac{1}{a_{11}} & 0 & ... & 0 \\ 
            0 & \frac{1}{a_{22}} & ... & 0 \\ 
            0 & 0 & ... &  \frac{1}{a_{nn}}
        \end{bmatrix} $
    \end{solution}
    
    \question
    \textbf{15. } (a) Show that a matrix with a row of zeroes cannot have an inverse. 


    \hspace{7.5mm} (b) Show that a matrix with a column of zeroes cannot have an inverse.
    \begin{solution}
        
    \end{solution}

    \question
    \textbf{16. } Is the sum of two invertible matrices necessarily invertible?
    \begin{solution}
        
    \end{solution}

    \question
    \textbf{17. } Let $A$ and $B$ be square matrices such that $ AB = 0 $. Show that if $A$ is invertible, then $B$ = 0. 
    \begin{solution}
        
    \end{solution}

    \question
    \textbf{29. } (a) Show that if $A$ is invertible and $ AB = AC $, then $ B = C $.


    \hspace{7.5mm} (b) Explain why part (a) and Example 3 (from the book) do not contradict one another.
    \begin{solution}
        
    \end{solution}

\end{questions}

\end{document}